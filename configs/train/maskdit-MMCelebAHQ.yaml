data:
  dataset: MMCelebAHQ
  category: lmdb
  resolution: 256
  num_channels: 3
  random_flip: True
  root: data/MM_CelebA_HQ/images/
  feat_path: None

model:
  precond: edm
  model_type: DiT-XL/2
  in_size: 32
  in_channels: 4 
  num_classes: 1
  use_decoder: True
  ext_feature_dim: 0
  pad_cls_token: False
  mask_ratio: 0.5
  mask_ratio_fn: constant
  mask_ratio_min: 0
  mae_loss_coef: 0.1
  class_dropout_prob: 0.1 # 1 for uncoditional
  self_cond: False

train:
  tf32: False
  amp: True
  batchsize: 128   # batchsize per GPU
  grad_accum: 1
  epochs: 10_000
  lr: 0.0001
  lr_rampup_kimg: 0
  xflip: False
  max_num_steps: 2_000_000

eval: # FID evaluation
  batchsize: 50
  ref_path: data/MM_CelebA_HQ/fid_refs/all_fid_ref.npz
  fid_sample_size: 256 # testset size

log:
  log_every: 50
  ckpt_every: 10_000 #12_500
  tag: pretrain
  use_tensorboard: True